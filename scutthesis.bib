% Encoding: UTF-8

@Article{seq2sql17,
  author  = {Victor Zhong and Caiming Xiong and Richard Socher},
  journal = {ArXiv},
  title   = {Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning},
  year    = {2017},
  volume  = {abs/1709.00103},
}

@InProceedings{spider18,
  author    = {Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and Zhang, Zilin and Radev, Dragomir},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  title     = {{S}pider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task},
  year      = {2018},
  address   = {Brussels, Belgium},
  month     = oct #{-} # nov,
  pages     = {3911--3921},
  publisher = {Association for Computational Linguistics},
  abstract  = {We present \textit{Spider}, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7{\%} exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at \url{https://yale-lily.github.io/seq2sql/spider}.},
  doi       = {10.18653/v1/D18-1425},
  url       = {https://www.aclweb.org/anthology/D18-1425},
}

@InProceedings{sparc19,
  author    = {Yu, Tao and Zhang, Rui and Yasunaga, Michihiro and Tan, Yi Chern and Lin, Xi Victoria and Li, Suyi and Er, Heyang and Li, Irene and Pang, Bo and Chen, Tao and Ji, Emily and Dixit, Shreya and Proctor, David and Shim, Sungrok and Kraft, Jonathan and Zhang, Vincent and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  title     = {{SP}ar{C}: Cross-Domain Semantic Parsing in Context},
  year      = {2019},
  address   = {Florence, Italy},
  month     = jul,
  pages     = {4511--4523},
  publisher = {Association for Computational Linguistics},
  abstract  = {We present SParC, a dataset for cross-domainSemanticParsing inContext that consists of 4,298 coherent question sequences (12k+ individual questions annotated with SQL queries). It is obtained from controlled user interactions with 200 complex databases over 138 domains. We provide an in-depth analysis of SParC and show that it introduces new challenges compared to existing datasets. SParC demonstrates complex contextual dependencies, (2) has greater semantic diversity, and (3) requires generalization to unseen domains due to its cross-domain nature and the unseen databases at test time. We experiment with two state-of-the-art text-to-SQL models adapted to the context-dependent, cross-domain setup. The best model obtains an exact match accuracy of 20.2{\%} over all questions and less than10{\%} over all interaction sequences, indicating that the cross-domain setting and the con-textual phenomena of the dataset present significant challenges for future research. The dataset, baselines, and leaderboard are released at https://yale-lily.github.io/sparc.},
  doi       = {10.18653/v1/P19-1443},
  url       = {https://www.aclweb.org/anthology/P19-1443},
}

@InProceedings{edit19,
  author    = {Zhang, Rui and Yu, Tao and Er, Heyang and Shim, Sungrok and Xue, Eric and Lin, Xi Victoria and Shi, Tianze and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  title     = {Editing-Based {SQL} Query Generation for Cross-Domain Context-Dependent Questions},
  year      = {2019},
  address   = {Hong Kong, China},
  month     = nov,
  pages     = {5338--5349},
  publisher = {Association for Computational Linguistics},
  abstract  = {We focus on the cross-domain context-dependent text-to-SQL generation task. Based on the observation that adjacent natural language questions are often linguistically dependent and their corresponding SQL queries tend to overlap, we utilize the interaction history by editing the previous predicted query to improve the generation quality. Our editing mechanism views SQL as sequences and reuses generation results at the token level in a simple manner. It is flexible to change individual tokens and robust to error propagation. Furthermore, to deal with complex table structures in different domains, we employ an utterance-table encoder and a table-aware decoder to incorporate the context of the user utterance and the table schema. We evaluate our approach on the SParC dataset and demonstrate the benefit of editing compared with the state-of-the-art baselines which generate SQL from scratch. Our code is available at \url{https://github.com/ryanzhumich/sparc_atis_pytorch}.},
  doi       = {10.18653/v1/D19-1537},
  url       = {https://www.aclweb.org/anthology/D19-1537},
}

@InProceedings{gnn19,
  author    = {Bogin, Ben and Berant, Jonathan and Gardner, Matt},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  title     = {Representing Schema Structure with Graph Neural Networks for Text-to-{SQL} Parsing},
  year      = {2019},
  address   = {Florence, Italy},
  month     = jul,
  pages     = {4560--4565},
  publisher = {Association for Computational Linguistics},
  abstract  = {Research on parsing language to SQL has largely ignored the structure of the database (DB) schema, either because the DB was very simple, or because it was observed at both training and test time. In spider, a recently-released text-to-SQL dataset, new and complex DBs are given at test time, and so the structure of the DB schema can inform the predicted SQL query. In this paper, we present an encoder-decoder semantic parser, where the structure of the DB schema is encoded with a graph neural network, and this representation is later used at both encoding and decoding time. Evaluation shows that encoding the schema structure improves our parser accuracy from 33.8{\%} to 39.4{\%}, dramatically above the current state of the art, which is at 19.7{\%}.},
  doi       = {10.18653/v1/P19-1448},
  url       = {https://www.aclweb.org/anthology/P19-1448},
}

@Article{ratsql19,
  author  = {Bailin Wang and Richard Shin and Xiaodong Liu and Oleksandr Polozov and Margot Richardson},
  journal = {ArXiv},
  title   = {RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers},
  year    = {2019},
  volume  = {abs/1911.04942},
}

@InProceedings{ggcn19,
  author    = {Bogin, Ben and Gardner, Matt and Berant, Jonathan},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  title     = {Global Reasoning over Database Structures for Text-to-{SQL} Parsing},
  year      = {2019},
  address   = {Hong Kong, China},
  month     = nov,
  pages     = {3659--3664},
  publisher = {Association for Computational Linguistics},
  abstract  = {State-of-the-art semantic parsers rely on auto-regressive decoding, emitting one symbol at a time. When tested against complex databases that are unobserved at training time (zero-shot), the parser often struggles to select the correct set of database constants in the new database, due to the local nature of decoding. {\%}since their decisions are based on weak, local information only. In this work, we propose a semantic parser that globally reasons about the structure of the output query to make a more contextually-informed selection of database constants. We use message-passing through a graph neural network to softly select a subset of database constants for the output query, conditioned on the question. Moreover, we train a model to rank queries based on the global alignment of database constants to question words. We apply our techniques to the current state-of-the-art model for Spider, a zero-shot semantic parsing dataset with complex databases, increasing accuracy from 39.4{\%} to 47.4{\%}.},
  doi       = {10.18653/v1/D19-1378},
  url       = {https://www.aclweb.org/anthology/D19-1378},
}

@InProceedings{coarse-to-fine18,
  author    = {Dong, Li and Lapata, Mirella},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Coarse-to-Fine Decoding for Neural Semantic Parsing},
  year      = {2018},
  address   = {Melbourne, Australia},
  month     = jul,
  pages     = {731--742},
  publisher = {Association for Computational Linguistics},
  abstract  = {Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.},
  doi       = {10.18653/v1/P18-1068},
  url       = {https://www.aclweb.org/anthology/P18-1068},
}

@Article{lstm97,
  author  = {Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal = {Neural Computation},
  title   = {Long Short-Term Memory},
  year    = {1997},
  pages   = {1735-1780},
  volume  = {9},
}

@InProceedings{attn17,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, undefinedukasz and Polosukhin, Illia},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  title     = {Attention is All You Need},
  year      = {2017},
  address   = {Red Hook, NY, USA},
  pages     = {6000–6010},
  publisher = {Curran Associates Inc.},
  series    = {NIPS’17},
  isbn      = {9781510860964},
  location  = {Long Beach, California, USA},
  numpages  = {11},
}

@InProceedings{wang2018execution-guided,
author = {Wang, Chenglong and Huang, Po-Sen and Polozov, Alex and Brockschmidt, Marc and , Rishabh Singh},
title = {Execution-Guided Neural Program Decoding},
booktitle = {ICML Neural Abstract Machines \& Program Induction workshop, 2018},
year = {2018},
month = {July},
abstract = {We present a neural semantic parser that translates natural language questions into executable SQL queries with two key ideas. First, we develop an encoder-decoder model, where the decoder uses a simple type system of SQL to constraint the output prediction, and propose a value-based loss when copying from input tokens. Second, we explore using the execution semantics of SQL to re-pair decoded programs that result in runtime error return empty result. We propose two model-agnostics repair approaches, an ensemble model and a local program repair, and demonstrate their effectiveness over the original model. We evaluate our model on the WikiSQL dataset and show that our model achieves close to state-of-the-art results with lesser model complexity.},
url = {https://www.microsoft.com/en-us/research/publication/execution-guided-neural-program-decoding/},
edition = {ICML Neural Abstract Machines \& Program Induction workshop, 2018},
}

@InProceedings{cho-etal-2014-properties,
  author    = {Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  booktitle = {Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
  title     = {On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches},
  year      = {2014},
  address   = {Doha, Qatar},
  month     = oct,
  pages     = {103--111},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/W14-4012},
  url       = {https://www.aclweb.org/anthology/W14-4012},
}

@InProceedings{bert19,
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  month     = jun,
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  doi       = {10.18653/v1/N19-1423},
  url       = {https://www.aclweb.org/anthology/N19-1423},
}

@InProceedings{yu-etal-2018-syntaxsqlnet,
  author    = {Yu, Tao and Yasunaga, Michihiro and Yang, Kai and Zhang, Rui and Wang, Dongxu and Li, Zifan and Radev, Dragomir},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  title     = {{S}yntax{SQLN}et: Syntax Tree Networks for Complex and Cross-Domain Text-to-{SQL} Task},
  year      = {2018},
  address   = {Brussels, Belgium},
  month     = oct #{-} # nov,
  pages     = {1653--1663},
  publisher = {Association for Computational Linguistics},
  abstract  = {Most existing studies in text-to-SQL tasks do not require generating complex SQL queries with multiple clauses or sub-queries, and generalizing to new, unseen databases. In this paper we propose SyntaxSQLNet, a syntax tree network to address the complex and cross-domain text-to-SQL generation task. SyntaxSQLNet employs a SQL specific syntax tree-based decoder with SQL generation path history and table-aware column attention encoders. We evaluate SyntaxSQLNet on a new large-scale text-to-SQL corpus containing databases with multiple tables and complex SQL queries containing multiple SQL clauses and nested queries. We use a database split setting where databases in the test set are unseen during training. Experimental results show that SyntaxSQLNet can handle a significantly greater number of complex SQL examples than prior work, outperforming the previous state-of-the-art model by 9.5{\%} in exact matching accuracy. To our knowledge, we are the first to study this complex text-to-SQL task. Our task and models with the latest updates are available at \url{https://yale-lily.github.io/seq2sql/spider}.},
  doi       = {10.18653/v1/D18-1193},
  url       = {https://www.aclweb.org/anthology/D18-1193},
}

@Article{Adam14,
  author  = {Kingma, Diederik and Ba, Jimmy},
  journal = {International Conference on Learning Representations},
  title   = {Adam: A Method for Stochastic Optimization},
  year    = {2014},
  month   = {12},
}

@InCollection{pytorch19,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
}

@InCollection{XLNet19,
  author    = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  title     = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {5753--5763},
  url       = {http://papers.nips.cc/paper/8812-xlnet-generalized-autoregressive-pretraining-for-language-understanding.pdf},
}

@InProceedings{cspider19,
  author    = {Min, Qingkai and Shi, Yuefeng and Zhang, Yue},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  title     = {A Pilot Study for {C}hinese {SQL} Semantic Parsing},
  year      = {2019},
  address   = {Hong Kong, China},
  month     = nov,
  pages     = {3652--3658},
  publisher = {Association for Computational Linguistics},
  abstract  = {The task of semantic parsing is highly useful for dialogue and question answering systems. Many datasets have been proposed to map natural language text into SQL, among which the recent Spider dataset provides cross-domain samples with multiple tables and complex queries. We build a Spider dataset for Chinese, which is currently a low-resource language in this task area. Interesting research questions arise from the uniqueness of the language, which requires word segmentation, and also from the fact that SQL keywords and columns of DB tables are typically written in English. We compare character- and word-based encoders for a semantic parser, and different embedding schemes. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL.},
  doi       = {10.18653/v1/D19-1377},
  url       = {https://www.aclweb.org/anthology/D19-1377},
}

@Article{LayerN16,
  author  = {Jimmy Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  journal = {ArXiv},
  title   = {Layer Normalization},
  year    = {2016},
  volume  = {abs/1607.06450},
}

@inproceedings{irnet19,
    title = "Towards Complex Text-to-{SQL} in Cross-Domain Database with Intermediate Representation",
    author = "Guo, Jiaqi  and
      Zhan, Zecheng  and
      Gao, Yan  and
      Xiao, Yan  and
      Lou, Jian-Guang  and
      Liu, Ting  and
      Zhang, Dongmei",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1444",
    doi = "10.18653/v1/P19-1444",
    pages = "4524--4535",
    abstract = "We present a neural approach called IRNet for complex and cross-domain Text-to-SQL. IRNet aims to address two challenges: 1) the mismatch between intents expressed in natural language (NL) and the implementation details in SQL; 2) the challenge in predicting columns caused by the large number of out-of-domain words. Instead of end-to-end synthesizing a SQL query, IRNet decomposes the synthesis process into three phases. In the first phase, IRNet performs a schema linking over a question and a database schema. Then, IRNet adopts a grammar-based neural model to synthesize a SemQL query which is an intermediate representation that we design to bridge NL and SQL. Finally, IRNet deterministically infers a SQL query from the synthesized SemQL query with domain knowledge. On the challenging Text-to-SQL benchmark Spider, IRNet achieves 46.7{\%} accuracy, obtaining 19.5{\%} absolute improvement over previous state-of-the-art approaches. At the time of writing, IRNet achieves the first position on the Spider leaderboard.",
}
